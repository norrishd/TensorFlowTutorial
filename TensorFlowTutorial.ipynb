{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorials\n",
    "### A boring work-through of the tutorials\n",
    "David Norrish, 2/10/2017  \n",
    "\n",
    "Working through of the TensorFlow tutorials, with a couple of extra MatPlotLib visualisations added. Best just for beginner reference (possibly just my own).\n",
    "\n",
    "Assumes you already have TensorFlow and (obviously) Jupyter Notebook installed  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial from https://www.tensorflow.org/get_started/mnist/beginners\n",
    "\n",
    "Firstly, import the package to give Python access to all of TensorFlow's classes, methods, and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **tensor** is a set of primitive values shaped into an array of any number of dimensions. A tensor's rank is its number of dimensions. E.g.\n",
    "\n",
    "- 3 \n",
    "  - a rank 0 tensor; this is a scalar with shape []\n",
    "- [1., 2., 3.] \n",
    "  - a rank 1 tensor; this is a vector with shape [3]\n",
    "- [[1., 2., 3.], [4., 5., 6.]] \n",
    "  - a rank 2 tensor; a matrix with shape [2, 3]\n",
    "- [[[1., 2., 3.]], [[7., 8., 9.]]]\n",
    "  - a rank 3 tensor with shape [2, 1, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'computational graph' is a series of TensorFlow operations arranged into a graph of nodes. Each node takes one or more tensor as inputs and produces a tensor as an output. A node can be a constant, which takes no inputs and stores its output value internally.  \n",
    "\n",
    "TensorFlow's Core programs essentially consist of two discrete sections:\n",
    "\n",
    "- Building the computational graph\n",
    "- Running the computational graph  \n",
    "\n",
    "Let's build a simple computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create 2 floating point tensors to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these constants are nodes that, when evaluated, would produce 3.0 and 4.0.  \n",
    "TF relies on a highly efficient C++ backend to do its computation. The connection to this backend is called a **session**.  \n",
    "  \n",
    "TF evaluates nodes by run the computational graph within a session, which encapsulates the control and state of the TensorFlow runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can build more complicated computations by combining Tensor nodes with operations (Operations are also nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3:\", node3)\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We only have constants at the moment, which is boring. A graph can be parameterized to accept external inputs, known as placeholders. A placeholder is a promise to provide a value later. Let's use these for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this more complex by adding another operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b: 4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables allow us to add **trainable parameters** to a graph.  \n",
    "\n",
    "W is the weights of the model, and b is the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike constants, to initialize all the variables in a TensorFlow program, you must explicitly call a special operation, `init`. This is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call `sess.run()`, the variables are uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`x` is a placeholder, so can evaluate `linear_model` for several values of x simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a model, but we don't know how good it is yet. To evaluate the model on training data, we need a y placeholder to provide the desired values, and we need to write a *loss* function.  \n",
    "This measures how far apart the current model is from the provided data. We'll use a standard loss model for linear regression, which sums the squares of the deltas between the current model and the provided data.  \n",
    "  \n",
    "`(linear_model - y)` creates a vector where each element is the corresponding example's error delta. We call `tf.square` to square that error. Then, we sum all the squared errors to create a single scalar that abstracts the error of all examples using `tf.reduce_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random choices of W and b clearly weren't perfect. We can re-assign them to their optimal values of -1 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "# I guess similar to running global_variables_initializer()\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, our loss is now 0. That's great, but this is machine learning! We want to **find** the parameter values. TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function. The simplest optimizer is gradient descent.  \n",
    "  \n",
    "*Gradient descent modifies each variable according to the magnitude of the derivative of loss with respect to that variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.21999997], dtype=float32), array([-0.456], dtype=float32)]\n",
      "[array([-0.84270465], dtype=float32), array([ 0.53753263], dtype=float32)]\n",
      "[array([-0.95284992], dtype=float32), array([ 0.86137295], dtype=float32)]\n",
      "[array([-0.98586655], dtype=float32), array([ 0.95844597], dtype=float32)]\n",
      "[array([-0.99576342], dtype=float32), array([ 0.98754394], dtype=float32)]\n",
      "[array([-0.99873012], dtype=float32), array([ 0.99626648], dtype=float32)]\n",
      "[array([-0.99961936], dtype=float32), array([ 0.99888098], dtype=float32)]\n",
      "[array([-0.99988592], dtype=float32), array([ 0.9996646], dtype=float32)]\n",
      "[array([-0.99996579], dtype=float32), array([ 0.99989945], dtype=float32)]\n",
      "[array([-0.99998969], dtype=float32), array([ 0.99996972], dtype=float32)]\n",
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "# Recall that 'loss' is the sum of squared error function\n",
    "sess.run(init) # reset values to incorrect defaults.\n",
    "W_vals, b_vals, loss_vals = [], [], []\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "    # keep track of value changes as we go to graph in next step\n",
    "    W_vals.append(sess.run(W))\n",
    "    b_vals.append(sess.run(b))\n",
    "    loss_vals.append(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))\n",
    "    if i % 100 == 0:\n",
    "        print(sess.run([W, b]))\n",
    "\n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/zDZNuIWFWjmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2483c930cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_vals)\n",
    "plt.plot(W_vals)\n",
    "plt.plot(b_vals)\n",
    "plt.legend(['loss score', 'W = gradient', 'b = intercept'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with a learning rate of 0.01 for the gradient descent optimizer, it takes about 200 generations for the loss score to asymptote to be very close to 0, and about an extra 200 generations for W and b to converge on essentially their final values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete trainable linear model, in all its stand-alone glory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # set values to wrong\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.estimator\n",
    "tf.estimator is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "- running training loops\n",
    "- running evaluation loops\n",
    "- managing data sets\n",
    "\n",
    "Basic usage for the previous problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\User\\AppData\\Local\\Temp\\tmpjbzggxy7\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\tmpjbzggxy7', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpjbzggxy7\\model.ckpt.\n",
      "INFO:tensorflow:loss = 14.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 800.096\n",
      "INFO:tensorflow:loss = 0.439799, step = 101 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 914.248\n",
      "INFO:tensorflow:loss = 0.115915, step = 201 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.6\n",
      "INFO:tensorflow:loss = 0.0136125, step = 301 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1010.54\n",
      "INFO:tensorflow:loss = 0.000627999, step = 401 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 914.234\n",
      "INFO:tensorflow:loss = 0.000770994, step = 501 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.6\n",
      "INFO:tensorflow:loss = 0.000115608, step = 601 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.62\n",
      "INFO:tensorflow:loss = 5.83211e-05, step = 701 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 914.228\n",
      "INFO:tensorflow:loss = 1.54388e-05, step = 801 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.6\n",
      "INFO:tensorflow:loss = 2.50322e-06, step = 901 (0.109 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpjbzggxy7\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 7.58401e-07.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-08-06:13:37\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpjbzggxy7\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-08-06:13:38\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.42914e-07, global_step = 1000, loss = 5.71657e-07\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-08-06:13:39\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpjbzggxy7\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-08-06:13:40\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.0025595, global_step = 1000, loss = 0.010238\n",
      "train metrics: {'average_loss': 1.4291433e-07, 'loss': 5.7165732e-07, 'global_step': 1000}\n",
      "eval metrics: {'average_loss': 0.0025594977, 'loss': 0.010237991, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "\n",
    "# Declare list of features. We only have one numeric feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# linear classification, and many neural network classifiers and regressors.\n",
    "# The following code provides an estimator that does linear regression.\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the method and passing the\n",
    "# training data set.\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A custom `estimator` model\n",
    "We can still retain the high level abstraction of data set, feeding, training, etc. of `tf.estimator`, while creating a custom model that is not built into TensorFlow.  \n",
    "  \n",
    "`tf.estimator.LinearRegressor` is actually a sub-class of `tf.estimator.Estimator`. Instead of sub-classing `Estimator`, we simply provide `Estimator` a function `model_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\User\\AppData\\Local\\Temp\\tmpogkf5nj1\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\tmpogkf5nj1', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpogkf5nj1\\model.ckpt.\n",
      "INFO:tensorflow:loss = 3.15136617367, step = 1\n",
      "INFO:tensorflow:global_step/sec: 914.26\n",
      "INFO:tensorflow:loss = 0.0118320647215, step = 101 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.95\n",
      "INFO:tensorflow:loss = 0.0019633222891, step = 201 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 914.234\n",
      "INFO:tensorflow:loss = 0.000359623509284, step = 301 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.61\n",
      "INFO:tensorflow:loss = 6.64644507019e-05, step = 401 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.6\n",
      "INFO:tensorflow:loss = 2.81870452566e-06, step = 501 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 914.234\n",
      "INFO:tensorflow:loss = 2.00310857453e-07, step = 601 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.62\n",
      "INFO:tensorflow:loss = 7.07771644979e-09, step = 701 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.6\n",
      "INFO:tensorflow:loss = 1.18861503091e-09, step = 801 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 914.246\n",
      "INFO:tensorflow:loss = 1.16596945719e-10, step = 901 (0.109 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpogkf5nj1\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.37179241132e-12.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-08-06:13:45\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpogkf5nj1\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-08-06:13:45\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1.0024e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-08-06:13:46\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpogkf5nj1\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-08-06:13:47\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101004\n",
      "train metrics: {'loss': 1.0023986e-11, 'global_step': 1000}\n",
      "eval metrics: {'loss': 0.010100389, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features, we only have one real-valued feature, 'labels'\n",
    "def model_fn(features, labels, mode):\n",
    "    # Build a linear model and predict values\n",
    "    W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "    b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "    y = W * features['x'] + b\n",
    "    # Loss sub-graph\n",
    "    loss = tf.reduce_sum(tf.square(y - labels))\n",
    "    # Training sub-graph\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "    # EstimatorSpec connects subgraphs we built to the\n",
    "    # appropriate functionality.\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=y,\n",
    "        loss=loss,\n",
    "        train_op=train)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# train\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MNIST for ML beginners\n",
    "Tutorial available at https://www.tensorflow.org/get_started/mnist/beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Modified National Institute of Standards and Technology database (MNIST) is a large database of handwritten digits that is commonly used for training various image processing systems. It is hosted on [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). Analysing it is the ML equivalent of Hello World.  \n",
    "  \n",
    "MNIST is split into three parts:\n",
    "- 55,000 data points of training data (mnist.train)\n",
    "- 10,000 points of test data (mnist.test)\n",
    "- 5,000 points of validation data (mnist.validation)\n",
    "  \n",
    "Each image is 28 pixels by 28 pixels. We can flatten this into a vector of 28x28 = 784 floats, each from [0, 1].\n",
    "  \n",
    "Flattening the data throws away information about the 2D structure of the image. This is kind of bad, but we'll start with a very simple model called a SoftMax regression, which doesn't use this structure anyway.  \n",
    "  \n",
    "The result is mnist.train.images will be a tensor (an n-dimensional array) with a shape of [55000, 784].  \n",
    "We're going to want our labels as \"one-hot vectors\", a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the n<sup>th</sup> digit will be represented as a vector which is 1 in the n<sup>th</sup> dimension (e.g. 3 would be [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Read in this sweet, sweet data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.\n",
    "\n",
    "It has two steps:\n",
    "1. add up the evidence of our input being in certain classes\n",
    "2. convert that evidence into probabilities\n",
    "  \n",
    "To tally up the evidence that a given image is in a particular class, we do a weighted sum of the pixel \n",
    "intensities. The weight is negative if that pixel having a high intensity is evidence against the image being in that class, and positive if it is evidence in favor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some extra evidence called a bias. Basically, we want to be able to say that some things are more likely independent of the input. The result is that the evidence for a class *i* given an input *x* is:\n",
    "\n",
    "evidence<sub>i</sub> = &Sigma;<sub>j</sub> (W<sub>i, j</sub> x<sub>j</sub> + b<sub>i</sub>)  \n",
    "  \n",
    "Where W<sub>i</sub> is the weights and b<sub>i</sub> is the bias for class *i* and *j* is an index for summing over our input image *x*  \n",
    "  \n",
    "We then convert the evidence tallies into our predicted probabilities *y* using the \"softmax\" function:  \n",
    "\n",
    "y = softmax(evidence)  \n",
    "  \n",
    "Here softmax is serving as an \"activation\" or \"link\" function, shaping the output of our linear function into the form we want -- in this case, a probability distribution over 10 cases. I.e. it converts tallies of evidence into probabilities of our input being in each class. Definition:  \n",
    "  \n",
    "softmax(x)<sub>i</sub> = exp(x<sub>i</sub>) / &Sigma;<sub>j</sub> exp(x<sub>j</sub>)  \n",
    "  \n",
    "Softmax exponentiates its inputs and then normalizes them. The exponentiation means that one more unit of evidence increases the weight given to any hypothesis multiplicatively. And conversely, having one less unit of evidence means that a hypothesis gets a fraction of its earlier weight. No hypothesis ever has zero or negative weight.  \n",
    "  \n",
    "Essentially we'll be doing:  \n",
    "y = softmax(W<sub>x</sub> + b)  \n",
    "  \n",
    "Well, I'm thosoughly confused. Time to get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the regression\n",
    "TensorFlow does its heavy lifting outside Python using highly efficient code implemented in another language, but it's still to switch back to Python after every operation. To avoid this, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Allow inputting any number of MNIST images, each flattened into a 784-dimensional vector\n",
    "# 'None' means a dimension can be of any length\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation. For machine learning applications, one generally has the model parameters be Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise both variables as 0s. We are going to learn them so doesn't matter what they start as\n",
    "# 784-dimensional image vectors by 10-dimensional vectors of evidence for the difference classes\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the model in a single line!  \n",
    "Multiply x by W with the expression `tf.matmul(x, W)`  \n",
    "This is flipped from when we multiplied them in our equation, where we had W<sub>x</sub>, as a small trick to deal with x being a 2D tensor with multiple inputs.  \n",
    "  \n",
    "Then add b (and call softmax, unless using the more stable version commented out below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We'll use \"cross entropy\" to determine our loss function. It arises from thinking about information compressing codes in information theory and is useful everywhere.  \n",
    "  \n",
    "H<sub>y'</sub> = -&Sigma;<sub>i</sub> y'<sub>i</sub> log(y<sub>i</sub>)  \n",
    "  \n",
    "Where y is our predicted probability distribution, and y' is the true distribution (the one-hot vector with the digit labels).  \n",
    "  \n",
    "To implement cross-entropy we need to first add a new placeholder to input the correct answers, then implement the cross-entropy function.  \n",
    "  \n",
    "First, `tf.log` computes the logarithm of each element of y. Next, we multiply each element of y_ with the corresponding element of `tf.log(y)`. Then `tf.reduce_sum` adds the elements in the second dimension of y, due to the `reduction_indices=[1]` parameter. Finally, `tf.reduce_mean` computes the mean over all the examples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# not numerically stable, but functionally what's happening, and carried out properly by tf.nn function below:\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we want our model to do, it's very easy to have TensorFlow train it to do so. Because TensorFlow knows the entire graph of your computations, it can automatically use the backpropagation algorithm to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss.  \n",
    "  \n",
    "Note that an **InteractiveSession** makes TensorFlow more flexible about how you structure your code. It allows you to interleave operations which build a computation graph with ones that run the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate of 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "# launch the model in an interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "# Initialise the variables created\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step of the loop, we'll get a \"batch\" of one hundred random data points from our training set. We run `train_step` feeding in the batches data to replace the placeholders.  \n",
    "  \n",
    "Using small batches of random data is called stochastic training -- in this case, stochastic gradient descent. Ideally, we'd like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that's expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to train - let's run 1000 times\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "First figure out where we predicted the correct label. `tf.argmax` is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, `tf.argmax(y,1)` is the label our model thinks is most likely for each input, while `tf.argmax(y_,1)` is the correct label. We can use `tf.equal` to check if our prediction matches the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, [True, False, True, True] would become [1,0,1,1] which would become 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's our overall accuracy - 92%. This isn't bad, certainly not great (the best get up around 99.7% accuracy). We'll get closer to that score in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep MNIST for experts\n",
    "Tutorial at: https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we construct a deep convolutional neural network for MNIST classification, and improve on the performance of the simpler model just implemented.  \n",
    "\n",
    "The tasks covered are:\n",
    "\n",
    "- Create a softmax regression function that is a model for recognizing MNIST digits, based on looking at every pixel in the image\n",
    "- Use Tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples (and run our first Tensorflow session to do so)\n",
    "- Check the model's accuracy with our test data\n",
    "- Build, train, and test a multilayer convolutional neural network to improve the results  \n",
    "\n",
    "`mnist` is a lightweight class which stores the training, validation, and testing sets as NumPy arrays. It also provides a function for iterating through data minibatches, which we will use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Read in the MNIST data again\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TF and initialise an InteractiveSession, which lets you interleave\n",
    "# building the computation graph with ones that run the graph\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build a softmax regression model again with a single linear layer, then extend this to a softmax with multilayer convolutional network.  \n",
    "  \n",
    "The input images x will consist of a 2d tensor of floating point numbers. Here we assign it a shape of [None, 784], where 784 is the dimensionality of a single flattened 28 by 28 pixel MNIST image, and None indicates that the first dimension, corresponding to the batch size, can be of any size. The target output classes y_ will also consist of a 2d tensor, where each row is a one-hot 10-dimensional vector indicating which digit class (zero through nine) the corresponding MNIST image belongs to.  \n",
    "\n",
    "Create nodes for the input images and target output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the weight W and bias b of the model.  \n",
    "\n",
    "A Variable is a value that lives in TensorFlow's computation graph. It can be used and even modified by the computation. In machine learning applications, one generally has the model parameters be Variables.  \n",
    "\n",
    "Remember, **W** is the weights, and **b** is the biases. Initialises them as 0s, as they shall be learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement out regression model. Multiply the vectorized input images x by the weight matrix W and add the bias b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now specify a loss function. Use the stable formulation to compute cross-entropy.  \n",
    "\n",
    "`tf.nn.softmax_cross_entropy_with_logits` internally applies the softmax on the model's unnormalized model prediction and sums across all classes, and tf.reduce_mean takes the average over these sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use automatic differentiation to find the gradients of the loss with respect to each of the variables. Step length of 0.5. This specifically adds new operations to the computation graph, which compute gradients, compute parameter update steps, and apply update steps to the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train the model.  \n",
    "\n",
    "Load 100 training examples in each iteration. Note you can replace any tensor in the computing graph with `feed_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model.  \n",
    "\n",
    "`tf.argmax` gives you the index of the highest entry in a tensor along some axis. E.g. `tf.argmax(y,1)` is the label our model thinks is most likely for each input. `tf.argmax(y_,1)` is the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives a list of booleans. Cast it to floating point numbers then take the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9176\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, get a result of ~92% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a multilayer convolutional network\n",
    "92% is a bad result. We will build a network that passes through the following layers to boost that performance substantially:\n",
    "1. reshape (weight initialization)\n",
    "2. convolutional 1\n",
    "3. pooling 1\n",
    "4. convolutional 2\n",
    "5. pooling 2\n",
    "6. fully-connected 1\n",
    "7. dropout\n",
    "8. fully-connected 2\n",
    "9. loss -> accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Weight initialization\n",
    "Good to generally initialize weights with a small amount of noise for symmetry breaking and to prevent 0 gradients. We're using 'rectified linear unit' (ReLU) neurons.  \n",
    "\n",
    "These neurons are biological inspired, mathematically justified, and are more effective in convolutional networks alternatives such as the logistic sigmoid or hyperbolic tangent. As of 2015 it is [the most popular activation function for deep neural networks](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)).  \n",
    "\n",
    "Because ReLU neurons have 0 activation if their input is <= 0, they should be initialized with random positive weights.  \n",
    "\n",
    "Define two functions that can be used repeatedly while building the model to generate starting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolution and pooling\n",
    "TensorFlow gives a lot of flexibility in convolution and pooling operations. Can set:  \n",
    "- How to handle the boundaries\n",
    "- Stride size\n",
    "\n",
    "Here we'll stick to the vanilla version. Use a stride of one and zero-pad so that the output is the same size as the input. Pooling is plain old max pooling over 2x2 blocks. To keep the code cleaner, also abstract those operations into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First convolutional layer**  \n",
    "\n",
    "This will consist of convolution followed by max pooling. The convolution will compute 32 features for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32]. The first two dimensions are the patch size, the next is the number of input channels, and the last is the number of output channels. We will also have a bias vector with a component for each output channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the layer, first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool. The max_pool_2x2 method will reduce the image size to 14x14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second convolutional layer**  \n",
    "\n",
    "The second layer will have 64 features for each 5x5 patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Densely connected layer\n",
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dropout\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. TensorFlow's `tf.nn.dropout` op automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Readout layer\n",
    "Finally add a layer, just like for the one layer softmax regression above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial and error evaluation\n",
    "To train and evaluate it we will use code that is nearly identical to that for the simple one layer SoftMax network above.  \n",
    "\n",
    "The differences are:  \n",
    "\n",
    "- Replace the steepest gradient descent optimizer with the more sophisticated ADAM optimizer.\n",
    "- Include the additional parameter keep_prob in feed_dict to control the dropout rate.  \n",
    "\n",
    "Also use `tf.Session` rather than `tf.InteractiveSession`. This better separates the process of creating the graph (model specification) and the process of evaluating the graph (model fitting). It generally makes for cleaner code. The `tf.Session` is created within a `with` block so that it is automatically destroyed once the block is exited.  \n",
    "\n",
    "NOTE: running this code with 10,000 iterations may take quite some time depending on your processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.18\n",
      "step 100, training accuracy 0.78\n",
      "step 200, training accuracy 0.88\n",
      "step 300, training accuracy 0.94\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.98\n",
      "step 600, training accuracy 0.94\n",
      "step 700, training accuracy 0.92\n",
      "step 800, training accuracy 0.94\n",
      "step 900, training accuracy 0.96\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.94\n",
      "step 1300, training accuracy 0.92\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.98\n",
      "step 1800, training accuracy 0.94\n",
      "step 1900, training accuracy 0.94\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2300, training accuracy 0.96\n",
      "step 2400, training accuracy 1\n",
      "step 2500, training accuracy 0.98\n",
      "step 2600, training accuracy 0.96\n",
      "step 2700, training accuracy 0.94\n",
      "step 2800, training accuracy 0.98\n",
      "step 2900, training accuracy 0.96\n",
      "step 3000, training accuracy 1\n",
      "step 3100, training accuracy 1\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 0.96\n",
      "step 3400, training accuracy 0.98\n",
      "step 3500, training accuracy 0.96\n",
      "step 3600, training accuracy 1\n",
      "step 3700, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 3900, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4100, training accuracy 1\n",
      "step 4200, training accuracy 0.96\n",
      "step 4300, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4500, training accuracy 0.98\n",
      "step 4600, training accuracy 1\n",
      "step 4700, training accuracy 0.98\n",
      "step 4800, training accuracy 1\n",
      "step 4900, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5100, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5300, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5500, training accuracy 0.98\n",
      "step 5600, training accuracy 0.98\n",
      "step 5700, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 5900, training accuracy 1\n",
      "step 6000, training accuracy 0.98\n",
      "step 6100, training accuracy 0.98\n",
      "step 6200, training accuracy 1\n",
      "step 6300, training accuracy 1\n",
      "step 6400, training accuracy 0.96\n",
      "step 6500, training accuracy 0.98\n",
      "step 6600, training accuracy 1\n",
      "step 6700, training accuracy 1\n",
      "step 6800, training accuracy 0.98\n",
      "step 6900, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7100, training accuracy 0.98\n",
      "step 7200, training accuracy 1\n",
      "step 7300, training accuracy 0.98\n",
      "step 7400, training accuracy 0.98\n",
      "step 7500, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7700, training accuracy 1\n",
      "step 7800, training accuracy 0.98\n",
      "step 7900, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8100, training accuracy 0.98\n",
      "step 8200, training accuracy 1\n",
      "step 8300, training accuracy 0.98\n",
      "step 8400, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 8600, training accuracy 0.98\n",
      "step 8700, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 8900, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9100, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9300, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9500, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# Softmax layer, as described in detail in the previous 2 sections\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "accuracy_evolution = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            accuracy_evolution.append(train_accuracy)         # store to plot afterwards\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(accuracy_evolution)\n",
    "plt.legend(['loss score'], loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
