{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorials\n",
    "### A boring work-through of the tutorials\n",
    "David Norrish, 2/10/2017  \n",
    "  \n",
    "Available at: https://www.tensorflow.org/get_started/get_started  \n",
    "Assumes you already have TensorFlow and (obviously) Jupyter Notebook installed  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, import the package to give Python access to all of TensorFlow's classes, methods, and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **tensor** is a set of primitive values shaped into an array of any number of dimensions. A tensor's rank is its number of dimensions. E.g.\n",
    "\n",
    "- 3 \n",
    "  - a rank 0 tensor; this is a scalar with shape []\n",
    "- [1., 2., 3.] \n",
    "  - a rank 1 tensor; this is a vector with shape [3]\n",
    "- [[1., 2., 3.], [4., 5., 6.]] \n",
    "  - a rank 2 tensor; a matrix with shape [2, 3]\n",
    "- [[[1., 2., 3.]], [[7., 8., 9.]]]\n",
    "  - a rank 3 tensor with shape [2, 1, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'computational graph' is a series of TensorFlow operations arranged into a graph of nodes. Each node takes one or more tensor as inputs and produces a tensor as an output. A node can be a constant, which takes no inputs and stores its output value internally.  \n",
    "\n",
    "TensorFlow's Core programs essentially consist of two discrete sections:\n",
    "\n",
    "- Building the computational graph\n",
    "- Running the computational graph  \n",
    "\n",
    "Let's build a simple computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create 2 floating point tensors to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these constants are nodes that, when evaluated, would product 3.0 and 4.0.  \n",
    "To evaluate the nodes, must run the computational graph within a **session**. A session encapsulates the control and state of the TensorFlow runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can build more complicated computations by combining Tensor nodes with operations (Operations are also nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3:\", node3)\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TensorFlow provides a utility called 'TensorBoard' that can display a picture of the computational graph. We only have constants at the moment, which is boring. A graph can be parameterized to accept external inputs, known as placeholders. A placeholder is a promise to provide a value later. Let's use these for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this more complex by adding another operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b: 4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables allow us to add trainable parameters to a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike constants, to initialize all the variables in a TensorFlow program, you must explicitly call a special operation,`init`. This is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call `sess.run()`, the variables are uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`x` is a placeholder, so can evaluate `linear_model` for several values of x simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a model, but we don't know how good it is yet. To evaluate the model on training data, we need a y placeholder to provide the desired values, and we need to write a *loss* function.  \n",
    "This measures how far apart the current model is from the provided data. We'll use a standard loss model for linear regression, which sums the squares of the deltas between the current model and the provided data.  \n",
    "  \n",
    "`linear_model - y` creates a vector where each element is the corresponding example's error delta. We call `tf.square` to square that error. Then, we sum all the squared errors to create a single scalar that abstracts the error of all examples using `tf.reduce_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random choices of W and b clearly weren't perfect. We can re-assign them to their optimal values of -1 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "# actually update the assignment, I guess?\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, our loss is now 0. That's great, but this is machine learning! We want to **find** the parameter values. TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function. The simplest optimizer is gradient descent.  \n",
    "  \n",
    "*Gradient descent modifies each variable according to the magnitude of the derivative of loss with respect to that variable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.21999997], dtype=float32), array([-0.456], dtype=float32)]\n",
      "[array([-0.84270465], dtype=float32), array([ 0.53753263], dtype=float32)]\n",
      "[array([-0.95284992], dtype=float32), array([ 0.86137295], dtype=float32)]\n",
      "[array([-0.98586655], dtype=float32), array([ 0.95844597], dtype=float32)]\n",
      "[array([-0.99576342], dtype=float32), array([ 0.98754394], dtype=float32)]\n",
      "[array([-0.99873012], dtype=float32), array([ 0.99626648], dtype=float32)]\n",
      "[array([-0.99961936], dtype=float32), array([ 0.99888098], dtype=float32)]\n",
      "[array([-0.99988592], dtype=float32), array([ 0.9996646], dtype=float32)]\n",
      "[array([-0.99996579], dtype=float32), array([ 0.99989945], dtype=float32)]\n",
      "[array([-0.99998969], dtype=float32), array([ 0.99996972], dtype=float32)]\n",
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "  if i % 100 == 0:\n",
    "    print(sess.run([W, b]))\n",
    "\n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete trainable linear model, in all its stand alone glory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # set values to wrong\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.estimator\n",
    "tf.estimator is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "- running training loops\n",
    "- running evaluation loops\n",
    "- managing data sets\n",
    "\n",
    "Basic usage for the previous problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\User\\AppData\\Local\\Temp\\tmpmkq2cejh\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\tmpmkq2cejh', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpmkq2cejh\\model.ckpt.\n",
      "INFO:tensorflow:loss = 14.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 581.845\n",
      "INFO:tensorflow:loss = 0.438845, step = 101 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.93\n",
      "INFO:tensorflow:loss = 0.113613, step = 201 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.6\n",
      "INFO:tensorflow:loss = 0.0135431, step = 301 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.6\n",
      "INFO:tensorflow:loss = 0.000626688, step = 401 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.62\n",
      "INFO:tensorflow:loss = 0.000770857, step = 501 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.63\n",
      "INFO:tensorflow:loss = 0.000115504, step = 601 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.92\n",
      "INFO:tensorflow:loss = 5.89776e-05, step = 701 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.61\n",
      "INFO:tensorflow:loss = 1.56348e-05, step = 801 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.62\n",
      "INFO:tensorflow:loss = 2.53982e-06, step = 901 (0.094 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpmkq2cejh\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 7.70526e-07.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-03-12:23:03\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpmkq2cejh\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-03-12:23:04\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.45228e-07, global_step = 1000, loss = 5.80913e-07\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-03-12:23:05\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpmkq2cejh\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-03-12:23:06\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00255981, global_step = 1000, loss = 0.0102393\n",
      "train metrics: {'average_loss': 1.452282e-07, 'loss': 5.809128e-07, 'global_step': 1000}\n",
      "eval metrics: {'average_loss': 0.0025598127, 'loss': 0.010239251, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "\n",
    "# Declare list of features. We only have one numeric feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# linear classification, and many neural network classifiers and regressors.\n",
    "# The following code provides an estimator that does linear regression.\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the method and passing the\n",
    "# training data set.\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A custom `estimator` model\n",
    "We can still retain the high level abstraction of data set, feeding, training, etc. of `tf.estimator`, while creating a custom model that is not built into TensorFlow.  \n",
    "  \n",
    "`tf.estimator.LinearRegressor` is actually a sub-class of `tf.estimator.Estimator`. Instead of sub-classing `Estimator`, we simply provide `Estimator` a function `model_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\User\\AppData\\Local\\Temp\\tmpqtrvhh89\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\tmpqtrvhh89', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpqtrvhh89\\model.ckpt.\n",
      "INFO:tensorflow:loss = 6.72097701409, step = 1\n",
      "INFO:tensorflow:global_step/sec: 914.375\n",
      "INFO:tensorflow:loss = 0.0246054524264, step = 101 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.62\n",
      "INFO:tensorflow:loss = 0.00172555450554, step = 201 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.96\n",
      "INFO:tensorflow:loss = 0.000354955778131, step = 301 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.91\n",
      "INFO:tensorflow:loss = 3.33150180595e-05, step = 401 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.63\n",
      "INFO:tensorflow:loss = 6.28694547245e-07, step = 501 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.91\n",
      "INFO:tensorflow:loss = 2.45419519271e-07, step = 601 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.96\n",
      "INFO:tensorflow:loss = 2.18817368154e-08, step = 701 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.92\n",
      "INFO:tensorflow:loss = 1.0011047552e-09, step = 801 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1279.95\n",
      "INFO:tensorflow:loss = 1.10681890685e-10, step = 901 (0.078 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\User\\AppData\\Local\\Temp\\tmpqtrvhh89\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.45908350016e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-03-12:23:09\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpqtrvhh89\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-03-12:23:10\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1.03831e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-03-12:23:11\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\User\\AppData\\Local\\Temp\\tmpqtrvhh89\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-03-12:23:12\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101007\n",
      "train metrics: {'loss': 1.0383119e-11, 'global_step': 1000}\n",
      "eval metrics: {'loss': 0.010100667, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features, we only have one real-valued feature, 'labels'\n",
    "def model_fn(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W * features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # EstimatorSpec connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# train\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MNIST for ML beginners\n",
    "The Modified National Institute of Standards and Technology database (MNIST) is a large database of handwritten digits that is commonly used for training various image processing systems. It is hosted on [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). Analysing it is the ML equivalent of Hello World.  \n",
    "  \n",
    "MNIST is split into three parts:\n",
    "- 55,000 data points of training data (mnist.train)\n",
    "- 10,000 points of test data (mnist.test)\n",
    "- 5,000 points of validation data (mnist.validation)\n",
    "  \n",
    "Each image is 28 pixels by 28 pixels. We can flatten this into a vector of 28x28 = 784 floats, each from [0, 1].\n",
    "  \n",
    "Flattening the data throws away information about the 2D structure of the image. This is kind of bad, but we'll start with a very simple model called a SoftMax regression, which doesn't use this structure anyway.  \n",
    "  \n",
    "The result is mnist.train.images will be a tensor (an n-dimensional array) with a shape of [55000, 784].  \n",
    "We're going to want our labels as \"one-hot vectors\", a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the n<sup>th</sup> digit will be represented as a vector which is 1 in the n<sup>th</sup> dimension (e.g. 3 would be [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Read in this sweet, sweet data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.\n",
    "\n",
    "It has two steps:\n",
    "1. add up the evidence of our input being in certain classes\n",
    "2. convert that evidence into probabilities\n",
    "  \n",
    "To tally up the evidence that a given image is in a particular class, we do a weighted sum of the pixel \n",
    "intensities. The weight is negative if that pixel having a high intensity is evidence against the image being in that class, and positive if it is evidence in favor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add some extra evidence called a bias. Basically, we want to be able to say that some things are more likely independent of the input. The result is that the evidence for a class *i* given an input *x* is:\n",
    "\n",
    "evidence<sub>i</sub> = &Sigma;<sub>j</sub> (W<sub>i, j</sub> x<sub>j</sub> + b<sub>i</sub>)  \n",
    "  \n",
    "Where W<sub>i</sub> is the weights and b<sub>i</sub> is the bias for class *i* and *j* is an index for summing over our input image *x*  \n",
    "  \n",
    "We then convert the evidence tallies into our predicted probabilities *y* using the \"softmax\" function:  \n",
    "\n",
    "y = softmax(evidence)  \n",
    "  \n",
    "Here softmax is serving as an \"activation\" or \"link\" function, shaping the output of our linear function into the form we want -- in this case, a probability distribution over 10 cases. I.e. it converts tallies of evidence into probabilities of our input being in each class. Definition:  \n",
    "  \n",
    "softmax(x)<sub>i</sub> = exp(x<sub>i</sub>) / &Sigma;<sub>j</sub> exp(x<sub>j</sub>)  \n",
    "  \n",
    "Softmax exponentiates its inputs and then normalizes them. The exponentiation means that one more unit of evidence increases the weight given to any hypothesis multiplicatively. And conversely, having one less unit of evidence means that a hypothesis gets a fraction of its earlier weight. No hypothesis ever has zero or negative weight.  \n",
    "  \n",
    "Essentially we'll be doing:  \n",
    "y = softmax(W<sub>x</sub> + b)  \n",
    "  \n",
    "Well, I'm thosoughly confused. Time to get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the regression\n",
    "TensorFlow does its heavy lifting outside Python using highly efficient code implemented in another language, but it's still to switch back to Python after every operation. To avoid this, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Allow inputting any number of MNIST images, each flattened into a 784-dimensional vector\n",
    "# 'None' means a dimension can be of any length\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation. For machine learning applications, one generally has the model parameters be Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise both variables as 0s. We are going to learn them so doesn't matter what they start as\n",
    "# 784-dimensional image vectors by 10-dimensional vectors of evidence for the difference classes\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the model in a single line!  \n",
    "Multiply x by W with the expression `tf.matmul(x, W)`  \n",
    "This is flipped from when we multiplied them in our equation, where we had W<sub>x</sub>, as a small trick to deal with x being a 2D tensor with multiple inputs.  \n",
    "  \n",
    "Then add b (and call softmax, unless using the more stable version commented out below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We'll use \"cross entropy\" to determine our loss function. It arises from thinking about information compressing codes in information theory and is useful everywhere.  \n",
    "  \n",
    "H<sub>y'</sub> = -&Sigma;<sub>i</sub> y'<sub>i</sub> log(y<sub>i</sub>)  \n",
    "  \n",
    "Where y is our predicted probability distribution, and y' is the true distribution (the one-hot vector with the digit labels).  \n",
    "  \n",
    "To implement cross-entropy we need to first add a new placeholder to input the correct answers, then implement the cross-entropy function.  \n",
    "  \n",
    "First, `tf.log` computes the logarithm of each element of y. Next, we multiply each element of y_ with the corresponding element of `tf.log(y)`. Then `tf.reduce_sum` adds the elements in the second dimension of y, due to the `reduction_indices=[1]` parameter. Finally, `tf.reduce_mean` computes the mean over all the examples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# not numerically stable, but functionally what's happening, and carried out properly by tf.nn function below:\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we want our model to do, it's very easy to have TensorFlow train it to do so. Because TensorFlow knows the entire graph of your computations, it can automatically use the backpropagation algorithm to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate of 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "# launch the model in an interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "# Initialise the variables created\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step of the loop, we'll get a \"batch\" of one hundred random data points from our training set. We run `train_step` feeding in the batches data to replace the placeholders.  \n",
    "  \n",
    "Using small batches of random data is called stochastic training -- in this case, stochastic gradient descent. Ideally, we'd like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that's expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to train - let's run 1000 times\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "First figure out where we predicted the correct label. `tf.argmax` is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, `tf.argmax(y,1)` is the label our model thinks is most likely for each input, while `tf.argmax(y_,1)` is the correct label. We can use `tf.equal` to check if our prediction matches the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, [True, False, True, True] would become [1,0,1,1] which would become 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9176\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's our overall accuracy - 92%. This isn't bad, certainly not great (the best get up around 99.7% accuracy). We'll get closer to that score in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep MNIST for experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
